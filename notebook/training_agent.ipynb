{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"training_agent.ipynb\"\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.wrappers import NormalizeObservation\n",
    "\n",
    "import wandb\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers.legacy import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import EpsGreedyQPolicy, LinearAnnealedPolicy\n",
    "from rl.callbacks import WandbLogger\n",
    "\n",
    "import game\n",
    "\n",
    "import importlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(game)\n",
    "\n",
    "env = game.Game()\n",
    "# env = NormalizeObservation(env)\n",
    "\n",
    "print(f\"{env.observation_space.shape}\")\n",
    "print(f\"{env.action_space.n}\")\n",
    "\n",
    "def build_model(input_shape, output_shape):\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(1,) + input_shape), \n",
    "        # Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(output_shape, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "input_shape = env.observation_space.shape\n",
    "output_shape = env.action_space.n\n",
    "\n",
    "model = build_model(input_shape, output_shape)\n",
    "print(model.summary())\n",
    "print(f\"{model.input_shape}\")\n",
    "print(f\"{model.output_shape}\")\n",
    "\n",
    "# Create a memory buffer\n",
    "memory = SequentialMemory(limit=20000, window_length=1)\n",
    "policy = LinearAnnealedPolicy(\n",
    "    EpsGreedyQPolicy(),\n",
    "    attr='eps', \n",
    "    value_max=1,\n",
    "    value_min=0.1,\n",
    "    value_test=0.05, \n",
    "    nb_steps=1000000\n",
    "    )\n",
    "# policy = EpsGreedyQPolicy(eps=0.1)\n",
    "\n",
    "# Define the Deep Q-Network agent\n",
    "dqn = DQNAgent(\n",
    "    model=model, \n",
    "    policy=policy,  \n",
    "    nb_actions=env.action_space.n, \n",
    "    memory=memory, \n",
    "    nb_steps_warmup=5000,\n",
    "    gamma=0.99,\n",
    "    target_model_update=10000,\n",
    "    enable_double_dqn=True, \n",
    "    )\n",
    "\n",
    "# Compile the model\n",
    "dqn.compile(\n",
    "    Adam(learning_rate=1e-3), \n",
    "    metrics=['mae']\n",
    "    )\n",
    "\n",
    "callbacks = [WandbLogger()]\n",
    "\n",
    "# Train the agent\n",
    "fit_history = dqn.fit(\n",
    "    env, \n",
    "    nb_steps=10000000, \n",
    "    action_repetition=1, \n",
    "    callbacks=callbacks,\n",
    "    verbose=1, \n",
    "    visualize=False, \n",
    "    nb_max_start_steps=0, \n",
    "    start_step_policy=None, \n",
    "    nb_max_episode_steps=None\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dqn.load_weights('dqn_weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(game)\n",
    "\n",
    "# env = game.Game()\n",
    "# env = NormalizeObservation(env)\n",
    "\n",
    "# Evaluate the agent\n",
    "test_history = dqn.test(\n",
    "    env, \n",
    "    nb_episodes=1, \n",
    "    action_repetition=1, \n",
    "    callbacks=None, \n",
    "    visualize=True, \n",
    "    nb_max_episode_steps=None, \n",
    "    nb_max_start_steps=0, \n",
    "    start_step_policy=None, \n",
    "    verbose=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('dqn_weights_run2.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
